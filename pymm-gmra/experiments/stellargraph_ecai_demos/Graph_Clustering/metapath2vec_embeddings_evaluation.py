# -*- coding: utf-8 -*-
"""metapath2vec-embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/embeddings/metapath2vec-embeddings.ipynb

# Node representation learning with Metapath2Vec

<table><tr><td>Run the latest release of this notebook:</td><td><a href="https://mybinder.org/v2/gh/stellargraph/stellargraph/master?urlpath=lab/tree/demos/embeddings/metapath2vec-embeddings.ipynb" alt="Open In Binder" target="_parent"><img src="https://mybinder.org/badge_logo.svg"/></a></td><td><a href="https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/embeddings/metapath2vec-embeddings.ipynb" alt="Open In Colab" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg"/></a></td></tr></table>

An example of implementing the Metapath2Vec representation learning algorithm using components from the `stellargraph` and `gensim` libraries.

**References**

**1.**  Metapath2Vec: Scalable Representation Learning for Heterogeneous Networks. Yuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 135â€“144, 2017. ([link](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf))

**2.** Distributed representations of words and phrases and their compositionality. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.  In Advances in Neural Information Processing Systems (NIPS), pp. 3111-3119, 2013. ([link](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf))

**3.** Gensim: Topic modelling for humans. ([link](https://radimrehurek.com/gensim/))

**4.** Social Computing Data Repository at ASU [http://socialcomputing.asu.edu]. R. Zafarani and H. Liu. Tempe, AZ: Arizona State University, School of Computing, Informatics and Decision Systems Engineering. 2009.
"""

# Commented out IPython magic to ensure Python compatibility.
# install StellarGraph if running on Google Colab
import sys
# if 'google.colab' in sys.modules:
#   %pip install -q stellargraph[demos]==1.2.1

# verify that we're using the correct version of StellarGraph for this notebook
import stellargraph as sg

try:
    sg.utils.validate_notebook_version("1.2.1")
except AttributeError:
    raise ValueError(
        f"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>."
    ) from None

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import os
import networkx as nx
import numpy as np
import pandas as pd
from stellargraph import datasets
from IPython.display import display, HTML

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score

from stellargraph.data import UniformRandomMetaPathWalk
from gensim.models import Word2Vec

"""## Load the dataset

(See [the "Loading from Pandas" demo](../basics/loading-pandas.ipynb) for details on how data can be loaded.)
"""

dataset = datasets.BlogCatalog3()
display(HTML(dataset.description))
g = dataset.load()
print(
    "Number of nodes {} and number of edges {} in graph.".format(
        g.number_of_nodes(), g.number_of_edges()
    )
)

walk_length = 100  # maximum length of a random walk to use throughout this notebook

# specify the metapath schemas as a list of lists of node types.
metapaths = [
    ["user", "group", "user"],
    ["user", "group", "user", "user"],
    ["user", "user"],
]

# Create the random walker
rw = UniformRandomMetaPathWalk(g)

walks = rw.run(
    nodes=list(g.nodes()),  # root nodes
    length=walk_length,  # maximum length of a random walk
    n=1,  # number of random walks per root node
    metapaths=metapaths,  # the metapaths
)

model = Word2Vec(walks, vector_size=128, window=5, min_count=0, sg=1, workers=2, epochs=1)

model.wv.vectors.shape  # 128-dimensional vector for each node in the graph

"""## Visualise Node Embeddings

We retrieve the Word2Vec node embeddings that are 128-dimensional vectors and then we project them down to 2 dimensions using the [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm.
"""

# Retrieve node embeddings and corresponding subjects
node_ids = model.wv.index_to_key  # list of node IDs
node_embeddings = (
    model.wv.vectors
)  # numpy.ndarray of size number of nodes times embeddings dimensionality
node_targets = [g.node_type(node_id) for node_id in node_ids]

# Define the file path where you want to save the node targets
file_path = "/scratch/f006dg0/stellargraph_ecai_demos/Graph_Clustering/results/node_targets.npy"
node_targets_array = np.array(node_targets)
np.save(file_path, node_targets_array)

# Orginal
# node_embeddings = "/scratch/f006dg0/experiments/Learned_embeddings/metapath2vec_embeddings.txt"
# Reduced
# node_embeddings = "/scratch/f006dg0/experiments/reduced_embeddings/metapath2vec_embeddings_dram.txt"
# print(node_embeddings[0])
# print(node_embeddings.shape)
# print(len(node_embeddings))

# Output the embeddings to a text file
output_dir = "/scratch/f006dg0/experiments/Unsupervised_learning/results"
output_path = os.path.join(output_dir, "metapath2vec_embeddings.txt")

# Create the directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Open the file in 'w' mode (write mode)
with open(output_path, 'w') as file:
    # Iterate over each node vector in model.wv.vectors
    for node_vector in model.wv.vectors:
        # Convert the node vector to a string with elements separated by spaces
        vector_str = " ".join(map(str, node_vector))
        # Write the node vector string to the file, followed by a newline character
        file.write(vector_str + "\n")

# Split the labeled nodes into a training set and a testing set
X_train, X_test, y_train, y_test = train_test_split(
    node_embeddings, node_targets, test_size=0.2, random_state=42
)

# Train a logistic regression classifier on the training set
clf = LogisticRegressionCV(
    Cs=10, cv=10, scoring="accuracy", verbose=False, multi_class="ovr", max_iter=1000
)
clf.fit(X_train, y_train)

# Predict the labels for the testing set
y_pred = clf.predict(X_test)

# Calculate the accuracy score
print(accuracy_score(y_test, y_pred))

# Results:
# Original: 0.9951714147754708
# Reduced: 
